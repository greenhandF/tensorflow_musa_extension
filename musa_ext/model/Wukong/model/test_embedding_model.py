import tensorflow as tf
from typing import List
import os

# --- 1. åŠ è½½ MUSA æ’ä»¶ ---
try:
    # è¯·ç¡®ä¿è·¯å¾„æŒ‡å‘ä½ ç¼–è¯‘å¥½çš„ so æ–‡ä»¶
    tf.load_op_library('/workspace/tensorflow_musa/build/libmusa_plugin.so')
    print(">>>> SUCCESS: MUSA plugin loaded. <<<<")
except Exception as e:
    print(f"Plugin Load Failed: {e}")

# --- 2. å…¼å®¹æ€§é…ç½® ---
# å…è®¸è½¯æ”¾ç½®ï¼šå¦‚æœ MUSA æ²¡å®ç°æŸä¸ªç®—å­ï¼Œè‡ªåŠ¨å›é€€åˆ° CPUï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­
tf.config.set_soft_device_placement(False)
# æ‰“å°ç®—å­åˆ†å¸ƒï¼šå¯ä»¥çœ‹åˆ°å“ªä¸ªç®—å­è·‘åœ¨ MUSAï¼Œå“ªä¸ªè·‘åœ¨ CPU
tf.debugging.set_log_device_placement(True)
# ==========================================
# 1. ä½ çš„æºç ï¼ˆä¿æŒä¸€æ¨¡ä¸€æ ·ï¼Œä¸åšä»»ä½•ä¿®æ”¹ï¼‰
# ==========================================
class SparseEmbedding(tf.keras.layers.Layer):
    def __init__(self, num_sparse_embs, dim_emb):
        super().__init__()
        self.embeddings = [
            tf.keras.layers.Embedding(input_dim=num_emb, output_dim=dim_emb)
            for num_emb in num_sparse_embs
        ]

    def call(self, sparse_inputs):
        sparse_outputs = [
            embedding(sparse_inputs[:, i])
            for i, embedding in enumerate(self.embeddings)
        ]
        return tf.stack(sparse_outputs, axis=1)


class Embedding(tf.keras.layers.Layer):
    def __init__(
        self,
        num_sparse_embs: List[int],
        dim_emb: int,
        dim_input_dense: int,
        bias: bool = False,
    ) -> None:
        super().__init__()

        self.dim_emb = dim_emb
        self.dim_input_dense = dim_input_dense

        self.sparse_embedding = SparseEmbedding(num_sparse_embs, dim_emb)
        self.dense_embedding = tf.keras.layers.Dense(
            units=dim_input_dense * dim_emb, use_bias=bias
        )

    def call(self, sparse_inputs: tf.Tensor, dense_inputs: tf.Tensor) -> tf.Tensor:
        sparse_outputs = self.sparse_embedding(sparse_inputs)

        dense_outputs = self.dense_embedding(dense_inputs)
        dense_outputs = tf.reshape(
            dense_outputs, [-1, self.dim_input_dense, self.dim_emb]
        )

        # concat along feature axis
        return tf.concat((sparse_outputs, dense_outputs), axis=1)

# ==========================================
# 2. è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
# ==========================================
def run_comprehensive_test():
    print("å¼€å§‹æµ‹è¯•åŸç‰ˆ Embedding æ¨¡å‹...")

    # é…ç½®å‚æ•°
    BATCH_SIZE = 4
    SPARSE_VOCAB_SIZES = [100, 200, 300, 400] # 4ä¸ªç¨€ç–ç‰¹å¾
    DENSE_FEATURE_COUNT = 5                  # 5ä¸ªå¯†é›†ç‰¹å¾
    EMB_DIM = 16

    # åˆå§‹åŒ–æ¨¡å‹
    model = Embedding(
        num_sparse_embs=SPARSE_VOCAB_SIZES,
        dim_emb=EMB_DIM,
        dim_input_dense=DENSE_FEATURE_COUNT
    )

    # æ„é€ æ¨¡æ‹Ÿæ•°æ®
    mock_sparse = tf.random.uniform((BATCH_SIZE, len(SPARSE_VOCAB_SIZES)), 0, 100, dtype=tf.int32)
    mock_dense = tf.random.normal((BATCH_SIZE, DENSE_FEATURE_COUNT))

    # --- æµ‹è¯•ç‚¹ 1: å‰å‘ä¼ æ’­ä¸å½¢çŠ¶æ ¡éªŒ ---
    print("\n[æµ‹è¯• 1] éªŒè¯å‰å‘ä¼ æ’­å½¢çŠ¶...")
    output = model(mock_sparse, mock_dense)
    
    # é¢„æœŸå½¢çŠ¶è®¡ç®—: 
    # Sparse(4 features) + Dense(5 features) = 9 total features
    # æ¯ä¸ª feature ç»´åº¦æ˜¯ 16
    # ç»“æœåº”ä¸º (4, 9, 16)
    expected_shape = [BATCH_SIZE, len(SPARSE_VOCAB_SIZES) + DENSE_FEATURE_COUNT, EMB_DIM]
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    assert list(output.shape) == expected_shape, f"å½¢çŠ¶é”™è¯¯ï¼é¢„æœŸ {expected_shape}"
    print("   âœ… å½¢çŠ¶æ ¡éªŒé€šè¿‡ã€‚")

    # --- æµ‹è¯•ç‚¹ 2: æ¢¯åº¦ä¸åå‘ä¼ æ’­ (éªŒè¯ MUSA ç®—å­ç¨³å®šæ€§) ---
    print("\n[æµ‹è¯• 2] éªŒè¯æ¢¯åº¦è®¡ç®— (Backward)...")
    with tf.GradientTape() as tape:
        res = model(mock_sparse, mock_dense)
        loss = tf.reduce_sum(res) # ç®€å•çš„æ±‚å’ŒæŸå¤±
    
    grads = tape.gradient(loss, model.trainable_variables)
    
    # æ£€æŸ¥æ‰€æœ‰å˜é‡æ˜¯å¦éƒ½æ‹¿åˆ°äº†æ¢¯åº¦
    for var, grad in zip(model.trainable_variables, grads):
        if grad is None:
            print(f"   âŒ é”™è¯¯: å˜é‡ {var.name} æœªè·å¾—æ¢¯åº¦ï¼")
            return
    print("   âœ… æ‰€æœ‰å‚æ•°æ¢¯åº¦è®¡ç®—æ­£å¸¸ã€‚")

    # --- æµ‹è¯•ç‚¹ 3: å†…éƒ¨ç»„ä»¶æ ¡éªŒ ---
    print("\n[æµ‹è¯• 3] éªŒè¯å­ç»„ä»¶ SparseEmbedding è¾“å‡º...")
    sparse_res = model.sparse_embedding(mock_sparse)
    assert sparse_res.shape == [BATCH_SIZE, len(SPARSE_VOCAB_SIZES), EMB_DIM]
    print(f"   Sparse å­æ¨¡å—è¾“å‡ºå½¢çŠ¶: {sparse_res.shape} âœ…")

    print("\n" + "="*30)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é¡¹å‡å·²é€šè¿‡ï¼æ¨¡å‹åœ¨å½“å‰ç¯å¢ƒä¸‹è¿è¡Œç¨³å®šã€‚")

if __name__ == "__main__":
    run_comprehensive_test()
