import tensorflow as tf
from tensorflow.keras.layers import Layer
import os

# --- 1. åŠ è½½ MUSA æ’ä»¶ ---
try:
    # è¯·ç¡®ä¿è·¯å¾„æŒ‡å‘ä½ ç¼–è¯‘å¥½çš„ so æ–‡ä»¶
    tf.load_op_library('/workspace/tensorflow_musa/build/libmusa_plugin.so')
    print(">>>> SUCCESS: MUSA plugin loaded. <<<<")
except Exception as e:
    print(f"Plugin Load Failed: {e}")

# --- 2. å…¼å®¹æ€§é…ç½® ---
# å…è®¸è½¯æ”¾ç½®ï¼šå¦‚æœ MUSA æ²¡å®ç°æŸä¸ªç®—å­ï¼Œè‡ªåŠ¨å›é€€åˆ° CPUï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­
tf.config.set_soft_device_placement(False)
# æ‰“å°ç®—å­åˆ†å¸ƒï¼šå¯ä»¥çœ‹åˆ°å“ªä¸ªç®—å­è·‘åœ¨ MUSAï¼Œå“ªä¸ªè·‘åœ¨ CPU
tf.debugging.set_log_device_placement(True)
# ==========================================
# 1. æºç å¤åˆ» (ä¿æŒä¸€æ¨¡ä¸€æ ·)
# ==========================================
class GELU(Layer):
    def __init__(self, **kwargs):
        super(GELU, self).__init__(**kwargs)

    def call(self, inputs):
        return 0.5 * inputs * (1.0 + tf.math.erf(inputs / tf.sqrt(2.0)))

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        config = super(GELU, self).get_config()
        return config


class MLP(tf.keras.Sequential):
    def __init__(
        self,
        dim_in: int,
        num_hidden: int,
        dim_hidden: int,
        dim_out: int,
        dropout: float = 0.0,
        bias: bool = False,
        activation: tf.keras.layers.Layer = GELU(),
    ) -> None:
        layers = []
        for _ in range(num_hidden - 1):
            layers.append(tf.keras.layers.Dense(units=dim_hidden, use_bias=bias))
            layers.append(tf.keras.layers.BatchNormalization())
            layers.append(activation)
            layers.append(tf.keras.layers.Dropout(dropout))
        layers.append(tf.keras.layers.Dense(units=dim_out, use_bias=bias))
        super().__init__(layers)

# ==========================================
# 2. è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
# ==========================================
def test_mlp_and_gelu():
    print("ğŸš€ å¼€å§‹æµ‹è¯• GELU æ¿€æ´»å‡½æ•°ä¸ MLP æ¨¡å‹...")

    # é…ç½®å‚æ•°
    BATCH_SIZE = 8
    DIM_IN = 128
    NUM_HIDDEN = 3    # 3å±‚æ„å‘³ç€ï¼š2ä¸ªéšè—å±‚å— + 1ä¸ªè¾“å‡ºå±‚
    DIM_HIDDEN = 256
    DIM_OUT = 1
    DROPOUT = 0.2

    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = MLP(
        dim_in=DIM_IN,
        num_hidden=NUM_HIDDEN,
        dim_hidden=DIM_HIDDEN,
        dim_out=DIM_OUT,
        dropout=DROPOUT,
        bias=True
    )

    # 2. æ„é€ æ¨¡æ‹Ÿè¾“å…¥
    mock_input = tf.random.normal((BATCH_SIZE, DIM_IN))

    # --- æµ‹è¯•ç‚¹ 1: å‰å‘ä¼ æ’­ä¸å½¢çŠ¶ ---
    print("\n[æµ‹è¯• 1] éªŒè¯å‰å‘ä¼ æ’­è¾“å‡ºå½¢çŠ¶...")
    output = model(mock_input, training=False)
    print(f"   è¾“å…¥å½¢çŠ¶: {mock_input.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    assert output.shape == (BATCH_SIZE, DIM_OUT), f"è¾“å‡ºå½¢çŠ¶é”™è¯¯ï¼Œé¢„æœŸ {(BATCH_SIZE, DIM_OUT)}"
    print("   âœ… å½¢çŠ¶æ ¡éªŒé€šè¿‡ã€‚")

    # --- æµ‹è¯•ç‚¹ 2: GELU ç®—å­æ•°å€¼åˆç†æ€§ ---
    print("\n[æµ‹è¯• 2] éªŒè¯ GELU æ¿€æ´»å‡½æ•°æ•°å€¼...")
    gelu_layer = GELU()
    # å½“ x=0 æ—¶ï¼ŒGELU(0) åº”ä¸º 0
    zero_test = gelu_layer(tf.constant([0.0]))
    # å½“ x å¾ˆå¤§æ—¶ï¼ˆå¦‚ 10.0ï¼‰ï¼ŒGELU(x) åº”è¯¥æ¥è¿‘ x
    large_test = gelu_layer(tf.constant([10.0]))
    
    print(f"   GELU(0): {zero_test.numpy()[0]:.4f}")
    print(f"   GELU(10): {large_test.numpy()[0]:.4f}")
    
    assert abs(zero_test.numpy()[0]) < 1e-6
    assert abs(large_test.numpy()[0] - 10.0) < 1e-4
    print("   âœ… GELU ç®—å­æ•°å€¼é€»è¾‘æ­£å¸¸ã€‚")

    # --- æµ‹è¯•ç‚¹ 3: æ¢¯åº¦åå‘ä¼ æ’­ (MUSA ç¨³å®šæ€§) ---
    print("\n[æµ‹è¯• 3] éªŒè¯åå‘ä¼ æ’­æ¢¯åº¦é“¾è·¯...")
    with tf.GradientTape() as tape:
        logits = model(mock_input, training=True)
        loss = tf.reduce_mean(tf.square(logits))
    
    grads = tape.gradient(loss, model.trainable_variables)
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å…¨ä¸º None
    has_none = False
    for var, grad in zip(model.trainable_variables, grads):
        if grad is None:
            print(f"   âŒ é”™è¯¯: å˜é‡ {var.name} ä¸¢å¤±æ¢¯åº¦ï¼")
            has_none = True
    
    if not has_none:
        print(f"   âœ… æ¢¯åº¦è®¡ç®—æ­£å¸¸ï¼Œå…±è·å– {len(grads)} ä¸ªå‚æ•°çš„æ¢¯åº¦ã€‚")

    print("\n" + "="*40)
    print("ğŸ‰ æ‰€æœ‰ MLP æºç ç›¸å…³æµ‹è¯•é¡¹å‡å·²é€šè¿‡ï¼")
    print("="*40)

if __name__ == "__main__":
    test_mlp_and_gelu()
