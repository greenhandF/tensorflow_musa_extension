import tensorflow as tf
import time
import os
from model.wukong import Wukong
plugin_path = "/workspace/tensorflow_musa/build/libmusa_plugin.so"
tf.load_library(plugin_path)
        
# å…è®¸è½¯æ”¾ç½®ï¼šå¦‚æœ MUSA æ²¡å®ç°æŸä¸ªç®—å­ï¼Œè‡ªåŠ¨å›é€€åˆ° CPUï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­
tf.config.set_soft_device_placement(True)
# æ‰“å°ç®—å­åˆ†å¸ƒï¼šå¯ä»¥çœ‹åˆ°å“ªä¸ªç®—å­è·‘åœ¨ MUSAï¼Œå“ªä¸ªè·‘åœ¨ CPU
tf.debugging.set_log_device_placement(True)


def test_wukong_training():
    print("\n" + "="*50)
    print("ğŸ”¥ æ­£åœ¨å¯åŠ¨ Wukong åå‘è®­ç»ƒæµ‹è¯•...")
    print("="*50)

    # 1. åŸºç¡€é…ç½®ï¼ˆä¸å‰æ–‡ä¿æŒä¸€è‡´ï¼‰
    batch_size = 16
    dim_emb = 16
    dim_input_sparse = 10
    dim_input_dense = 5
    num_sparse_embs = [1000] * dim_input_sparse
    
    model = Wukong(
        num_layers=2,
        num_sparse_embs=num_sparse_embs,
        dim_emb=dim_emb,
        dim_input_sparse=dim_input_sparse,
        dim_input_dense=dim_input_dense,
        num_emb_lcb=8,
        num_emb_fmb=4,
        rank_fmb=2,
        num_hidden_wukong=1,
        dim_hidden_wukong=32,
        num_hidden_head=2,
        dim_hidden_head=64,
        dim_output=1,
        dropout=0.1
    )

    # 2. å®šä¹‰è®­ç»ƒç»„ä»¶
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    loss_fn = tf.keras.losses.BinaryCrossentropy()

    # 3. æ„é€ ä¼ªé€ çš„è®­ç»ƒæ•°æ®å’Œæ ‡ç­¾
    sparse_data = tf.random.uniform((batch_size, dim_input_sparse), 0, 999, dtype=tf.int32)
    dense_data = tf.random.normal((batch_size, dim_input_dense))
    # æ¨¡æ‹ŸäºŒåˆ†ç±»æ ‡ç­¾ (0 æˆ– 1)
    labels = tf.cast(tf.random.uniform((batch_size, 1), 0, 2, dtype=tf.int32), tf.float32)

    # 4. å®šä¹‰å•æ­¥è®­ç»ƒé€»è¾‘ (ä½¿ç”¨ GradientTape)
    @tf.function  # ä½¿ç”¨å›¾æ¨¡å¼åŠ é€Ÿè®­ç»ƒ
    def train_step(s_data, d_data, y_true):
        with tf.GradientTape() as tape:
            # è¿è¡Œå‰å‘ä¼ æ’­
            y_pred = model([s_data, d_data], training=True)
            # è®¡ç®—æŸå¤±
            loss = loss_fn(y_true, y_pred)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = tape.gradient(loss, model.trainable_variables)
        # æ›´æ–°æƒé‡
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        return loss

    # 5. æ‰§è¡Œå¾ªç¯è®­ç»ƒæµ‹è¯•
    print(f"å¼€å§‹è®­ç»ƒæµ‹è¯•ï¼ˆæ‰§è¡Œ 5 ä¸ª Iterationsï¼‰...")
    for i in range(1, 6):
        start_time = time.time()
        loss_val = train_step(sparse_data, dense_data, labels)
        end_time = time.time()
        
        print(f"Iteration {i} | Loss: {loss_val.numpy():.4f} | Time: {(end_time-start_time)*1000:.2f}ms")

    print("\nâœ… åå‘è®­ç»ƒæµ‹è¯•å®Œæˆï¼æ¢¯åº¦æ›´æ–°æ­£å¸¸ã€‚")

if __name__ == "__main__":
    # ç¡®ä¿ MLP ç±»å’Œ Embedding ç±»å·²å®šä¹‰
    
    try:
        test_wukong_training()
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
